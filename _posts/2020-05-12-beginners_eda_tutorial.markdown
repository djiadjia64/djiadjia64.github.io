---
layout: post
title:      "Beginner's EDA Tutorial "
date:       2020-05-13 03:42:16 +0000
permalink:  beginners_eda_tutorial
---

In this short blog post, I will be covering some of the basics of cleaning up your dataset for analysis. Initially, EDA can be pretty vague of a task for a beginner to data science. Ultimately, you're trying to clean and visualize your dataset to make sure the data can be interpreted correctly. 

One of the first things to do is make sure all the data is consistent and entered in the same format. For example, making sure all of the numerical objects are correctly classifed. If a column has a string or is classified as a string, you can simply remove the string characters with a .replace(). After making sure the rows have no strings, you can turn the entire column into an int with astype(int) from (str). It is also important to see and visualize the shape of your dataset. How many variables, whether the columns are consistent in length, visualizing the values and counts of each column, etc... This can be done through visualizations with hist(), a visual graph, etc. If the column has too many missing values or is much shorter or longer than the other columns, it may prove wise to drop the column entirely. One easy way to see how many missing values a column has is through the isna().sum() method. This will return you a list of the number of missing values you have for each column. To see unique values and value count, you can call the methods .unique() and .value_counts() respectively. If a column has an acceptable amount of missing values, there are several tricks you can do to 'fill them in.' One common trick is to simply fill in the missing values with the median values of the entire row. The concept behind that is that you want to enter in whatever value appears the most often for consistency across the dataset. This median value can also be replaced with a mean or whichever classification you feel is appropriate for that particular type of data. Dropping entire rows of the dataset may also be the best idea only if you are not deleting too much data from the dataset itself. In my opinion, this is a 'last resort' kind of thing as this indiscriminately deletes from your dataset. The next step is making sure that the columns of the dataset can be properly interpeted by the model used for analysis. This means that the dataset must be normal for one. This means that a histogram will show a regular bell curve of the values in that particular column. If this is not the case and the curve slants to one side of another, a log function of the entire column may help correcting the skew. Normalizing or standardizing the dataset is also important. A machine learning model will weight whatever explicit number in the dataset is bigger so making sure there is an equal scale across variables is critical. Before, you may have had to create your own normalizer or standardizer but nowadays, there are built-in functions already there to help minimize this proces. StandardScaler can be imported and applied to your dataset, returning a numpy-array that has been normalized between -1 and 1. This process is extremely fast and greatly speeds up the EDA process, although you must add back in the index by yourself/convert the array to your dataset type of choice. 

This is pretty far into the process. By now, the data should be clean with no missing values and with all the correct types of values. The data should be normalized and should show normal characteristics on a bell curve plot. With these steps done, you can now work on transforming your data. Feature engineering is method that reduces variables by combining them or making new variables from existing ones. It is a pretty open-ended process that requires a bit of creative thinking. PCA can also be used to reduce dimensions. This is a way of combining variables in a way that makes machine learning models function a lot more efficiently and ultimately will be better at predicting. 

In this tutorial, you have learned about some of the basics when it comes to preparing a dataset for analysis. Initially, this can seem like a pretty daunting task as there is no definitive checklist when it comes to each dataset. Each and every dataset must be analyzed individually and cleaned accordingly. The data scientist must make a decision when it comes to which particular rows or colums to keep, whether columns should be changed with feature engineering or dimension-reduction with PCA. How to deal with missing values is also a big one and decisions can drastically change how the data is interpeted by the algorithm. All in all, this is an explorative process and one needs plenty of practice before really feeling fully comfortable with such a interpretive process.


