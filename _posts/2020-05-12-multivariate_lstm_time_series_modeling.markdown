---
layout: post
title:      "Multivariate LSTM Time Series Modeling"
date:       2020-05-12 23:14:53 -0400
permalink:  multivariate_lstm_time_series_modeling
---


Previously, I had worked with a similar model of just regular time-series modeling. In a normal time-series model, the model trains through time. Particularly, it emphasizes more recent data than older data and usually tends to return an average-move of the data points just previous of the prediction. Consequentially, there was not much nuance to its predictions, usually just returning a simple trend-following strategy with weighted averages, an already thorougly-explored indicator. However, it gave me an introductory glance into machine learning for forecasting and introduced me to more complex models. 

Specifically, I was looking for a model that could take different variables instead of just changing on time alone like in a time-series model. Also, I was looking for a model would not follow a simple moving-average strategy. This lead me into work with deeper neural networks and specifically something called LSTM ('Long Short-Term Memory) neural networks. In traditional neural networks, the model trains through weighting parameters and can return either a linear activation number or binary activation. However, these neural networks suffer from 'loss-of-memory' problems as there is no added 'input-gate' for memory. Consequentially, these models will base their results explicitly on recent data. An LSTM model fixes this flaw by creating a gate for 'memory.' It's pretty complicated in its details but basically inside the model is a gate that feeds back into the predictor gate that gives the neural network 'memory.' Understandably, this concept is vague and hard to grasp specifically unless you understand some of the math and layout of the neurons themselves but that isn't too important right now. The most important thing to take from this is that LSTMs are improved neural networks that are able to predict on memory better and can take more than one variable. 

These properties make it ideal for stock predicting in my opinion. There are many parameters that can change how a stock price fluctuates and pinpointing the important variables is critical. It would seem appropriate to run an unsupervised-learning model to see hidden patterns and correlations and then attempt to make a predictive model off of those significant variables. Perhaps this will be my next project, though acquiring enough unique yet significant data will be hard to find and costly at that. For this project, I wasn't able to get too in-depth into different kinds of parameters and hence used other stock or commodity prices along with the stock I was trying to predict. This is a kind of study on the correlations of different commodities and may reveal patterns in its price. Overall, I was attempting to predict the price of SPY (top 500 US companies) with the variables: TLT(bond yield measure), GLD (price of gold), and UVXY (volatility). In hindsight, UVXY was not a good choice as its historic price changed greatly due to stock-splits and what not. With more time, I would have adjusted the prices of UVXY to match throughout the ten years of data. 

Along with the initial predictive variables I had inputted, I also had to create a column of data of SPY price but pushed ahead. This is to give the LSTM a column to learn off of and predict off of. I decided with a time-shift of three days ultimately, as it would allow me to predict up to three days in the future, a more useful prediction than herhaps just one day. This concept seemed a little weird to be at first but you are trying to predict short-term into the future so it makes sense to create a training column similar to this type of forecast. Creating the layers itself was relatively straight-foward for this project. I only created one hidden layer, which could be optimized later on. I had a relatively high validation loss but it did decrease gradually as the model was trained. The accuracy was very good, but that makes sense as I was training the data on the data that I was actually testing on. Next time, I should use a different set of data for the validation data and the testing data. Also, I trained the model on a relative uptrend from the last ten years so the model may only work as intended if we stay in a similar trend in the long-term, which is of course not guaranteed. 

The performance of my model was good but was also cause I was testing on the validation data as well. I should have inputted data from a different period. However, the types of moves being forecasted were pretty good in that they were not linear in nature. I was not able to get the model forecasting correctly from a period of larger than three days, this will be my focus next time when and if I do a follow-up project on this experiment. All in all, however, LSTMs proved to be very interesting. With some model tuning, the accuracy is not bad at all. 
